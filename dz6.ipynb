{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0da9035-2e2a-46f0-868e-593cc42eed54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.18.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (4.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (26.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (7.2.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.48.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.67.3)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.2.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.7.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.36.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (23.0.1)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.6.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.9.86)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.21.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install peft datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03f16cac-9cd0-4642-9b51-0059b0062411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import peft\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30b76c91-cbb8-4507-9979-8e71aa08aab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "        print(f'VRAM allocated {allocated}gb, reserved {reserved}gb')\n",
    "    else:\n",
    "        print('No cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d6fc167-5ee0-46e0-8dcc-a76780b84d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff982d0823347d68eee64982c2a17fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8e2051311a4350800e03cebbb12ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#model_name = 'BioMistral/BioMistral-Safetensors'\n",
    "model_name = 'mistralai/Mistral-7B-v0.1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "#specials = [\"<|user|>\", \"<|assistant|>\", \"<|end|>\"]\n",
    "#tokenizer.add_special_tokens({\"additional_special_tokens\": specials})\n",
    "foundation_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                        torch_dtype=torch.bfloat16,\n",
    "                                                        device_map=\"auto\",\n",
    "                                                        load_in_4bit=True,\n",
    "                                                        attn_implementation=\"flash_attention_2\")\n",
    "foundation_model = prepare_model_for_kbit_training(foundation_model)\n",
    "foundation_model.gradient_checkpointing_enable()\n",
    "#foundation_model.resize_token_embeddings(len(tokenizer))\n",
    "foundation_model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5360da5-cc31-418c-8490-593ec792b8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_template exists: False\n",
      "chat_template preview:\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "print(\"chat_template exists:\", bool(getattr(tokenizer, \"chat_template\", None)))\n",
    "print(\"chat_template preview:\\n\", tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2f5a343-a573-42a1-b030-f3b071773e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template) # Проверка на поддержку чата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d70bc6f-baa0-496a-aec3-119edac36aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outputs(model, inputs, max_new_tokens=1000):\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        repetition_penalty=1.5,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b8f7a02-3552-45e9-8dc7-434f024d2a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VRAM allocated 4.662720203399658gb, reserved 5.197265625gb\n"
     ]
    }
   ],
   "source": [
    "print_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5fbacf6-8ad2-42b7-89d2-61afa293eed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"
     ]
    }
   ],
   "source": [
    "input_simple_sentences_text = 'I have high temperature. Which pills should I take? Answer with pills, no additional info. Pills:'\n",
    "input_simple_sentences = tokenizer(input_simple_sentences_text, return_tensors='pt').to(foundation_model.device)\n",
    "foundational_outputs_sentence = get_outputs(foundation_model, input_simple_sentences)\n",
    "output = tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "120617ac-ab96-49b6-a272-a4f4285ece48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I have high temperature. Which pills should I take? Answer with pills, no additional info. Pills:\\n1) 20 mg of prednisone (orally), every day for a week; then taper down by one pill per month until you’re off it completely in six months or so. This is the standard treatment and works well if your body can tolerate steroids without too many side effects like weight gain/moon face etc., but some people don't respond as quickly to this type of therapy because their immune system has been suppressed over time due lack proper nutrition from food sources that contain anti-inflammatory properties such as omega three fatty acids found naturally within fish oils which help reduce inflammation throughout our bodies including joints where arthritis often occurs when there isn't enough lubricant between bones causing them rub together creating pain sensations we feel all around us!\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8181a72-3806-4b5b-8895-54ebbd2e1301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "input_json_sentences_text = 'I have high temperature. Which pills should I take? Answer with json, contining pills. Example [\"carbamazepine\", \"lamotrigine\"]. Your answer in json: '\n",
    "input_json_sentences = tokenizer(input_json_sentences_text, return_tensors='pt').to(foundation_model.device)\n",
    "foundational_outputs_sentence = get_outputs(foundation_model, input_json_sentences)\n",
    "output = tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57abe848-2cf9-4b9d-896e-3d2f4d2bf43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have high temperature. Which pills should I take? Answer with json, contining pills. Example [\"carbamazepine\", \"lamotrigine\"]. Your answer in json:  [[\"Carisoprodol\"],\\nDIPHENHYDRAMINE HYDROCHLORIDE (Benedryl) is an antihistamine used to treat allergy symptoms such as itching and runny nose or sneezing; hives and other skin rashes caused by allergies; cough due to minor throat irritation from the common cold or flu...'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615825ce-b228-4f3a-aa5c-5ac69f9fb8fd",
   "metadata": {},
   "source": [
    "# Начинаем готовить LORA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e83f444-2d5a-4bd3-a2c3-a59721641c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'mlabonne/FineTome-100k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c58a2a6-6984-4e25-a777-a808f3dcc28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations', 'source', 'score'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(dataset_name)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a02ec10-8802-4001-9335-9700b9128fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_plain(sample):\n",
    "    conversation_plain = ''\n",
    "    conversations = sample['conversations']\n",
    "    for speech in conversations:\n",
    "        if speech['from'] == 'human':\n",
    "            conversation_plain = conversation_plain + '<|user|>\\n' + speech['value'] + '\\n'\n",
    "        if speech['from'] == 'gpt':\n",
    "            conversation_plain = conversation_plain + '<|assistant|>\\n' + speech['value'] + '\\n'\n",
    "\n",
    "    conversation_plain = conversation_plain + '<|end|>\\n'\n",
    "        \n",
    "    return {'conversation_plain': conversation_plain}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d65772dc-cd14-464e-8a06-9b36da63e8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(convert_to_plain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45ea3c27-ef28-4c4f-bf60-336f05a34bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations', 'source', 'score', 'conversation_plain'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e449a99-3a31-47c8-8422-dc6cdd8b5a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|user|>\\nPrint the reverse of a string using a for loop.\\n<|assistant|>\\nHere is an example code using a for loop to print the reverse of a string along with a detailed docstring:\\n\\n```python\\ndef print_reverse_string(string):\\n    \"\"\"\\n    Prints the reverse of a given string using a for loop.\\n\\n    Parameters:\\n    string (str): The string to be reversed and printed.\\n\\n    Returns:\\n    None.\\n    \"\"\"\\n\\n    # Iterate through the characters of the string in reverse order\\n    for i in range(len(string)-1, -1, -1):\\n        print(string[i], end=\\'\\')\\n\\n    # Print a new line after printing the whole reversed string\\n    print()\\n\\n# Example usage\\ninput_string = input(\"Enter a string: \")\\nprint_reverse_string(input_string)\\n```\\n\\nIn this code, the `print_reverse_string` function takes a string as input. It uses a for loop to iterate through the characters of the string in reverse order. The for loop starts from the index of the last character (`len(string)-1`) and goes until the first character (`-1`), with a step of `-1`. In each iteration, the function prints the character using the `print` function and `end=\\'\\'` is used to avoid printing the default newline character after each character. Finally, after printing the whole reversed string, `print()` is called to print a new line.\\n<|end|>\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['conversation_plain'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2803a3b-03d5-4fa3-b256-acde6a358b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(lambda samples: tokenizer(samples['conversation_plain']), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2463bd00-1e40-468c-ba11-eb6dcbb53463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations', 'source', 'score', 'conversation_plain', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66f1f3cb-82a1-412e-9c04-cc697e0fc208",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8, # As bigger the R bigger the parameters to train.\n",
    "    lora_alpha=16, # a scaling factor that adjusts the magnitude of the weight matrix. Usually set to 1\n",
    "    #target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\", 'gate_proj', 'up_proj', 'down_proj'],\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "    #modules_to_save=[\"embed_tokens\", \"lm_head\"],\n",
    "    lora_dropout=0.05, # Helps to avoid Overfitting.\n",
    "    #bias=\"lora_only\", # this specifies if the bias parameter should be trained.\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f3468b9-fe6a-48e5-a94d-ec3ccdcb0201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,815,744 || all params: 7,248,547,840 || trainable%: 0.0940\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(foundation_model, lora_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a110aac2-e10f-4a95-9622-eb6366542571",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = './'\n",
    "\n",
    "output_directory = os.path.join(working_dir, 'peft_lab_outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1844498-51f5-4444-8faf-98229a97d2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = output_directory,\n",
    "    auto_find_batch_size = True,\n",
    "    learning_rate = 1e-4,\n",
    "    num_train_epochs = 2,\n",
    "    use_cpu = False,\n",
    "    bf16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86b2dc9d-7acc-4f19-956f-f018913639e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data['train'].take(4000)\n",
    "train_dataset = train_dataset.remove_columns(['conversations', 'source', 'score', 'conversation_plain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca75d7f1-2062-4708-8176-0ac73b3fdc6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 4000\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626d2793-3af8-43ba-87e4-74d8b532f1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  27/2000 02:26 < 3:13:01, 0.17 it/s, Epoch 0.03/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = peft_model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80588754-4078-4d0c-a372-492c9c834abb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80927f9-7a65-4165-9dd7-6bcdbf19a66b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
