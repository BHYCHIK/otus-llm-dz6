{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0da9035-2e2a-46f0-868e-593cc42eed54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.18.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (4.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (26.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (7.2.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.48.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.67.3)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.2.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.7.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.36.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (23.0.1)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.6.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.9.86)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.21.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install peft datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03f16cac-9cd0-4642-9b51-0059b0062411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import peft\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30b76c91-cbb8-4507-9979-8e71aa08aab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "        print(f'VRAM allocated {allocated}gb, reserved {reserved}gb')\n",
    "    else:\n",
    "        print('No cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d6fc167-5ee0-46e0-8dcc-a76780b84d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e6be6bebd9745a7a96e739952787549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d47c065b34274533b7f77cbb22affcdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#model_name = 'BioMistral/BioMistral-Safetensors'\n",
    "model_name = 'mistralai/Mistral-7B-v0.1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "#specials = [\"<|user|>\", \"<|assistant|>\", \"<|end|>\"]\n",
    "#tokenizer.add_special_tokens({\"additional_special_tokens\": specials})\n",
    "foundation_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                        torch_dtype=torch.bfloat16,\n",
    "                                                        device_map=\"auto\",\n",
    "                                                        load_in_4bit=True,\n",
    "                                                        attn_implementation=\"flash_attention_2\")\n",
    "foundation_model = prepare_model_for_kbit_training(foundation_model)\n",
    "foundation_model.gradient_checkpointing_enable()\n",
    "#foundation_model.resize_token_embeddings(len(tokenizer))\n",
    "foundation_model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5360da5-cc31-418c-8490-593ec792b8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_template exists: False\n",
      "chat_template preview:\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "print(\"chat_template exists:\", bool(getattr(tokenizer, \"chat_template\", None)))\n",
    "print(\"chat_template preview:\\n\", tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2f5a343-a573-42a1-b030-f3b071773e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template) # Проверка на поддержку чата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d70bc6f-baa0-496a-aec3-119edac36aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outputs(model, inputs, tokenizer, max_new_tokens=1000):\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        repetition_penalty=1.5,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        tokenizer=tokenizer,\n",
    "        stop_strings=['<|end|>']\n",
    "    )\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b8f7a02-3552-45e9-8dc7-434f024d2a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VRAM allocated 4.662720203399658gb, reserved 5.197265625gb\n"
     ]
    }
   ],
   "source": [
    "print_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5fbacf6-8ad2-42b7-89d2-61afa293eed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"
     ]
    }
   ],
   "source": [
    "input_simple_sentences_text = 'I have high temperature. Which pills should I take? Answer with pills, no additional info. Pills:'\n",
    "input_simple_sentences = tokenizer(input_simple_sentences_text, return_tensors='pt').to(foundation_model.device)\n",
    "foundational_outputs_sentence = get_outputs(foundation_model, input_simple_sentences, tokenizer)\n",
    "output = tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "120617ac-ab96-49b6-a272-a4f4285ece48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I have high temperature. Which pills should I take? Answer with pills, no additional info. Pills:\\n1) 20 mg of prednisone (orally), every day for a week; then taper down by one pill per month until you’re off it completely in six months or so. This is the standard treatment and works well if your body can tolerate steroids without too many side effects like weight gain/moon face etc., but some people don't respond as quickly to this type of therapy because their immune system has been suppressed over time due lack proper nutrition from food sources that contain anti-inflammatory properties such as omega three fatty acids found naturally within fish oils which help reduce inflammation throughout our bodies including joints where arthritis often occurs when there isn't enough lubricant between bones causing them rub together creating pain sensations we feel all around us!\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8181a72-3806-4b5b-8895-54ebbd2e1301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "input_json_sentences_text = 'I have high temperature. Which pills should I take? Answer with json, contining pills. Example [\"carbamazepine\", \"lamotrigine\"]. Your answer in json: '\n",
    "input_json_sentences = tokenizer(input_json_sentences_text, return_tensors='pt').to(foundation_model.device)\n",
    "foundational_outputs_sentence = get_outputs(foundation_model, input_json_sentences, tokenizer)\n",
    "output = tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57abe848-2cf9-4b9d-896e-3d2f4d2bf43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have high temperature. Which pills should I take? Answer with json, contining pills. Example [\"carbamazepine\", \"lamotrigine\"]. Your answer in json:  [[\"Carisoprodol\"],\\nDIPHENHYDRAMINE HYDROCHLORIDE (Benedryl) is an antihistamine used to treat allergy symptoms such as itching and runny nose or sneezing; hives and other skin rashes caused by allergies; cough due to minor throat irritation from the common cold or flu...'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615825ce-b228-4f3a-aa5c-5ac69f9fb8fd",
   "metadata": {},
   "source": [
    "# Начинаем готовить LORA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e83f444-2d5a-4bd3-a2c3-a59721641c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'mlabonne/FineTome-100k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c58a2a6-6984-4e25-a777-a808f3dcc28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations', 'source', 'score'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(dataset_name)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a02ec10-8802-4001-9335-9700b9128fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_plain(sample):\n",
    "    conversation_plain = ''\n",
    "    conversations = sample['conversations']\n",
    "    for speech in conversations:\n",
    "        if speech['from'] == 'human':\n",
    "            conversation_plain = conversation_plain + '<|user|>\\n' + speech['value'] + '\\n'\n",
    "        if speech['from'] == 'gpt':\n",
    "            conversation_plain = conversation_plain + '<|assistant|>\\n' + speech['value'] + '\\n'\n",
    "\n",
    "    conversation_plain = conversation_plain + '<|end|>\\n'\n",
    "        \n",
    "    return {'conversation_plain': conversation_plain}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d65772dc-cd14-464e-8a06-9b36da63e8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(convert_to_plain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45ea3c27-ef28-4c4f-bf60-336f05a34bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations', 'source', 'score', 'conversation_plain'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e449a99-3a31-47c8-8422-dc6cdd8b5a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|user|>\\nPrint the reverse of a string using a for loop.\\n<|assistant|>\\nHere is an example code using a for loop to print the reverse of a string along with a detailed docstring:\\n\\n```python\\ndef print_reverse_string(string):\\n    \"\"\"\\n    Prints the reverse of a given string using a for loop.\\n\\n    Parameters:\\n    string (str): The string to be reversed and printed.\\n\\n    Returns:\\n    None.\\n    \"\"\"\\n\\n    # Iterate through the characters of the string in reverse order\\n    for i in range(len(string)-1, -1, -1):\\n        print(string[i], end=\\'\\')\\n\\n    # Print a new line after printing the whole reversed string\\n    print()\\n\\n# Example usage\\ninput_string = input(\"Enter a string: \")\\nprint_reverse_string(input_string)\\n```\\n\\nIn this code, the `print_reverse_string` function takes a string as input. It uses a for loop to iterate through the characters of the string in reverse order. The for loop starts from the index of the last character (`len(string)-1`) and goes until the first character (`-1`), with a step of `-1`. In each iteration, the function prints the character using the `print` function and `end=\\'\\'` is used to avoid printing the default newline character after each character. Finally, after printing the whole reversed string, `print()` is called to print a new line.\\n<|end|>\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['conversation_plain'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2803a3b-03d5-4fa3-b256-acde6a358b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(lambda samples: tokenizer(samples['conversation_plain']), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2463bd00-1e40-468c-ba11-eb6dcbb53463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations', 'source', 'score', 'conversation_plain', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66f1f3cb-82a1-412e-9c04-cc697e0fc208",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8, # As bigger the R bigger the parameters to train.\n",
    "    lora_alpha=16, # a scaling factor that adjusts the magnitude of the weight matrix. Usually set to 1\n",
    "    #target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\", 'gate_proj', 'up_proj', 'down_proj'],\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "    #modules_to_save=[\"embed_tokens\", \"lm_head\"],\n",
    "    lora_dropout=0.05, # Helps to avoid Overfitting.\n",
    "    #bias=\"lora_only\", # this specifies if the bias parameter should be trained.\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f3468b9-fe6a-48e5-a94d-ec3ccdcb0201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,815,744 || all params: 7,248,547,840 || trainable%: 0.0940\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(foundation_model, lora_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a110aac2-e10f-4a95-9622-eb6366542571",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = './'\n",
    "\n",
    "output_directory = os.path.join(working_dir, 'peft_lab_outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1844498-51f5-4444-8faf-98229a97d2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = output_directory,\n",
    "    auto_find_batch_size = True,\n",
    "    learning_rate = 1e-4,\n",
    "    num_train_epochs = 2,\n",
    "    use_cpu = False,\n",
    "    bf16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86b2dc9d-7acc-4f19-956f-f018913639e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data['train'].take(4000)\n",
    "train_dataset = train_dataset.remove_columns(['conversations', 'source', 'score', 'conversation_plain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca75d7f1-2062-4708-8176-0ac73b3fdc6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 4000\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "626d2793-3af8-43ba-87e4-74d8b532f1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='29' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  29/2000 02:38 < 3:12:44, 0.17 it/s, Epoch 0.03/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='226' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 226/4000 10:29 < 2:56:53, 0.36 it/s, Epoch 0.11/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8000' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8000/8000 3:14:54, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.631800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.708500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.711400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.697100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.685100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.677000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.669900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.555300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.544200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.547300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.559600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.548800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.553400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.557500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.539600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8000, training_loss=0.6154109001159668, metrics={'train_runtime': 11696.7115, 'train_samples_per_second': 0.684, 'train_steps_per_second': 0.684, 'total_flos': 2.349074502156288e+17, 'train_loss': 0.6154109001159668, 'epoch': 2.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = peft_model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    resume_from_checkpoint=True\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80588754-4078-4d0c-a372-492c9c834abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./peft_lab_outputs/lora_adapter_1/tokenizer_config.json',\n",
       " './peft_lab_outputs/lora_adapter_1/special_tokens_map.json',\n",
       " './peft_lab_outputs/lora_adapter_1/tokenizer.model',\n",
       " './peft_lab_outputs/lora_adapter_1/added_tokens.json',\n",
       " './peft_lab_outputs/lora_adapter_1/tokenizer.json')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter_dir = \"./peft_lab_outputs/lora_adapter_1\"\n",
    "\n",
    "peft_model.save_pretrained(adapter_dir)   # сохраняет только веса адаптера + config\n",
    "tokenizer.save_pretrained(adapter_dir)    # важно, если добавляли спец-токены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f7c4c8d9-c393-4419-99e7-3f2db647ed5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): MistralRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "779b3104-1382-4845-aad7-a02a347321b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_json_sentences_text = '''\n",
    "<|user|>\n",
    "I have low temperature. Which pills should I take? Answer with json, contining pills. Answer with json without additional information. Example [\"carbamazepine\", \"lamotrigine\"]. \n",
    "<|assistant|>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e80927f9-7a65-4165-9dd7-6bcdbf19a66b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'peft_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_json_sentences \u001b[38;5;241m=\u001b[39m tokenizer(input_json_sentences_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[43mpeft_model\u001b[49m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m      2\u001b[0m lora_outputs_sentence \u001b[38;5;241m=\u001b[39m get_outputs(peft_model, input_json_sentences, tokenizer)\n\u001b[1;32m      3\u001b[0m output \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(lora_outputs_sentence, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'peft_model' is not defined"
     ]
    }
   ],
   "source": [
    "input_json_sentences = tokenizer(input_json_sentences_text, return_tensors='pt').to(peft_model.device)\n",
    "lora_outputs_sentence = get_outputs(peft_model, input_json_sentences, tokenizer)\n",
    "output = tokenizer.batch_decode(lora_outputs_sentence, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "15b599a9-129e-4c1d-8bb8-9ab2b75aa263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n<|user|>\\nI have low temperature. Which pills should I take? Answer with json, contining pills. Answer with json without additional information. Example [\"carbamazepine\", \"lamotrigine\"]. \\n<|assistant|>\\n1) Start by understanding the problem: The user needs to know which medications they can use for treating their fever or other symptoms of a cold/flu virus infection (low body temperatures).  2) Identify what type(s) of medication would be appropriate in this case - antibiotics are not typically used as these types of viruses do not respond well to them; instead antiviral drugs may help reduce severity and duration if started early enough after onset but there is no cure yet so prevention through vaccination remains key strategy against spread among population at risk e.g., children under five years old who often suffer most severe effects due lack immunity built up over time from exposure during childhood when immune system develop normally .   3) List possible options based upon available evidence including clinical trials data showing efficacy rates per drug class depending factors such dosage frequency route administration etc.. For example one option could include acetaminophen combined paracetamol tablets taken every four hours until symptom relief achieved while another might involve taking ibuprofen capsules twice daily along side drinking plenty fluids staying hydrated throughout day avoiding alcohol consumption altogether since it dehydrates quickly leading worsening headaches nausea vomiting diarrheaa nd fatigue amongst others things associated influenza like illnesses especially those caused respiratory pathogens capable spreading rapidly within communities lacking adequate sanitation facilities access clean water supplies needed prevent further outbreak occurrences hereafter referred simply “pills” : [“Acetominapen”, ”Paracteanum Tabletus Taken Every Four Hours Until Symptoms Relief Achieved , ”ibupefuen Capsule Twice Daily Along Side Drinking Plenty Fluid Stay Hyderate Throughout Day Avoid Alcohol Consume Entirely Since It Dehydra Quickely Lead Worse Headache Nausia Vome Diarrea And Fatique Amongst Others Things Associated Influeza Like Illness Especially Those Causes Respirotary Pathogen Capable Spread Rapid Within Communities Lacking Adquate Sanitization Facilities Access Clean Water Supplies Needed Prevent Further Outbreak Occurrence Here After Referred Simply Pillis ]] End Result =[{\"name\":\"Acetominape\",\"dosages\":[\"Every Four Hour\"],\"side_effects\"[]}, {\"name\":\"Paraectamen Tabletas Tomadas Todo Cuatro Horas hasta el aliviamento del síntoma\"}, {...}]}\\n<|end|>']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29c5e186-4b6d-4908-bd4d-3d034eab7cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"
     ]
    }
   ],
   "source": [
    "input_json_sentences = tokenizer(input_json_sentences_text, return_tensors='pt').to(foundation_model.device)\n",
    "foundation_outputs_sentence = get_outputs(foundation_model, input_json_sentences, tokenizer)\n",
    "output = tokenizer.batch_decode(foundation_outputs_sentence, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ead4612-3402-473f-9435-bf7ae7b5a8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n<|user|>\\nI have low temperature. Which pills should I take? Answer with json, contining pills. Answer with json without additional information. Example [\"carbamazepine\", \"lamotrigine\"]. \\n<|assistant|>\\nYou can use the following: [pill_name] or you may also say it\\'s name in english (if available). For example if your pill is called \\'Lamitral\\', then just type lamiltral and we will find all possible matches for this word. If there are no results found - please try to search by its generic names like CarbaMazapin instead of Tegretol etc...']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f748c29-0bc6-4c16-a5be-e2a67fcf2e41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
